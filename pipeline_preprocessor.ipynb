{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the data pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style = 'white')\n",
    "# sns.set(style='whitegrid', color_codes=True)\n",
    "from tqdm import tqdm\n",
    "import plotly\n",
    "import time\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "import warnings\n",
    "import random\n",
    "import gc\n",
    "# import dask.dataframe as dd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import validation\n",
    "import re\n",
    "from sys import getsizeof\n",
    "import scipy\n",
    "from lightgbm import LGBMRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from pandas import Grouper\n",
    "from scipy.sparse import csr_matrix\n",
    "import joblib\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgbm\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', None)\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation,Dense,Flatten,BatchNormalization,Dropout,LeakyReLU,LSTM,Conv1D,TimeDistributed,MaxPool1D,Flatten,Input\n",
    "from tensorflow.keras.regularizers import l1,l2,l1_l2\n",
    "import tensorflow.keras.initializers as initializer\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import json\n",
    "tf.random.set_seed(120)\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "# rfered this blog\n",
    "# https://pythonhosted.org/calmap/\n",
    "# np.random.seed(sum(map(ord, 'calmap')))\n",
    "import calmap\n",
    "import pickle\n",
    "import joblib\n",
    "# from joblib import Parallel,delayed\n",
    "from scipy.stats import norm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "color_cycle = cycle([\"blue\",\"green\",\"red\",\"cyan\",\"magenta\",'yellow','black','darkorange','grey','lime'])\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's store some preprocessed data like lable encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "  incoming_df_mem_usage = df.memory_usage().sum() / 1024**2\n",
    "  print(\"Memory usage of properties dataframe is :\",incoming_df_mem_usage,\" MB\")\n",
    "  na_list = [] # to keep track the columns that have missing values filled in\n",
    "  for col in df.columns:\n",
    "    if df[col].dtype != object:\n",
    "      is_int = False\n",
    "      max =  df[col].max()\n",
    "      min = df[col].min()\n",
    "\n",
    "      # integer doesn't support nan, so it needs to be filled\n",
    "      if not np.isfinite(df[col].all()):\n",
    "        na_list.app(col)\n",
    "        df[col].fillna(mn-1,inplace=True)\n",
    "      # test if column can be converted to integer\n",
    "      asint = df[col].fillna(0).astype(np.int64)\n",
    "      result = (df[col] - asint)\n",
    "      result = result.sum()\n",
    "\n",
    "      if result > -0.01 and result < 0.01:\n",
    "        is_int = True\n",
    "      if is_int:\n",
    "        if min>=0:\n",
    "          if max < 255:\n",
    "            df[col] = df[col].astype(np.uint8)\n",
    "          elif max < 65535:\n",
    "            df[col] = df[col].astype(np.uint16)\n",
    "          elif max < 4294967295:\n",
    "            df[col] = df[col].astype(np.uint32)\n",
    "          else:\n",
    "            df[col] = df[col].astype(np.uint64)\n",
    "        else:\n",
    "          if min > np.iinfo(np.int8).min and max < np.iinfo(np.int8).max:\n",
    "            df[col] = df[col].astype(np.int8)\n",
    "          elif min > np.iinfo(np.int16).min and max < np.iinfo(np.int16).max:\n",
    "            df[col] = df[col].astype(np.int16)\n",
    "          elif min > np.iinfo(np.int32).min and max < np.iinfo(np.int32).max:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "          elif min > np.iinfo(np.int64).min and max < np.iinfo(np.int64).max:\n",
    "            df[col] = df[col].astype(np.int64)\n",
    "      else:\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "         \n",
    "  print(\"********************** MEMORY USAGE AFTER COMPLETION **********************\")\n",
    "  outgoing_df_mem_usage = df.memory_usage().sum() / 1024**2 \n",
    "  print(\"Memory usage is: \",outgoing_df_mem_usage,\" MB\")\n",
    "  print(\"This is \",100*outgoing_df_mem_usage/incoming_df_mem_usage,\"% of the initial size\")\n",
    "  return df, na_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 452.91172790527344  MB\n",
      "********************** MEMORY USAGE AFTER COMPLETION **********************\n",
      "Memory usage is:  65.71533966064453  MB\n",
      "This is  14.509524839327831 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "sales_train_evaluation = pd.read_csv(r\"sales_train_evaluation.csv\")  #The properties dataset\n",
    "sales_train_evaluation, na_list = reduce_mem_usage(sales_train_evaluation)\n",
    "if(len(na_list)!=0):\n",
    "  print(\"=\"*100)\n",
    "  print(\"Note: The following columns have missed values filled in with data_frame['column_name'].min() -1\")\n",
    "  print(\"=\"*100)\n",
    "  print(na_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 0.2104339599609375  MB\n",
      "********************** MEMORY USAGE AFTER COMPLETION **********************\n",
      "Memory usage is:  0.1221780776977539  MB\n",
      "This is  58.06005728373577 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "calendar = pd.read_csv(r\"calendar.csv\")  #The properties dataset\n",
    "calendar, na_list = reduce_mem_usage(calendar)\n",
    "if(len(na_list)!=0):\n",
    "  print(\"=\"*100)\n",
    "  print(\"Note: The following columns have missed values filled in with data_frame['column_name'].min() -1\")\n",
    "  print(\"=\"*100)\n",
    "  print(na_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 208.77456665039062  MB\n",
      "********************** MEMORY USAGE AFTER COMPLETION **********************\n",
      "Memory usage is:  143.5325527191162  MB\n",
      "This is  68.75001827184856 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "sell_prices = pd.read_csv(r\"sell_prices.csv\")  #The properties dataset\n",
    "sell_prices, na_list = reduce_mem_usage(sell_prices)\n",
    "if(len(na_list)!=0):\n",
    "  print(\"=\"*100)\n",
    "  print(\"Note: The following columns have missed values filled in with data_frame['column_name'].min() -1\")\n",
    "  print(\"=\"*100)\n",
    "  print(na_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have data for 1941 days, first 1913 for train dataset and 1914-1941 is validation dataset and we have to predict the sales for next 28 days so, we have add d_42 to d_1970 with values and make it as our test dataset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(1942,1970):\n",
    "    col = 'd_' + str(d)\n",
    "    sales_train_evaluation[col] = 0\n",
    "    sales_train_evaluation[col] = sales_train_evaluation[col].astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day_cols = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "# for d in range(1970-180,1970):\n",
    "#     col = 'd_' + str(d)\n",
    "#     day_cols.append(col)\n",
    "# sales_train_evaluation_test = sales_train_evaluation[day_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.melt(sales_train_evaluation, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='units_sold').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, calendar, on='d', how='left')\n",
    "df = pd.merge(df, sell_prices, on=['store_id','item_id','wm_yr_wk'], how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['day_of_month'] = df['date'].dt.day.astype('int8')\n",
    "df['week_of_month'] = df['date'].apply(lambda d: (d.day-1) // 7 + 1).astype('int8')\n",
    "df['week_of_year'] = df['date'].dt.week.astype('int8')\n",
    "df['quarter_of_year'] = df['date'].dt.quarter.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_month_start'] = [1 if i == 1 else 0 for i in df['week_of_month']]\n",
    "df['is_month_end'] = [1 if i in [4,5] else 0 for i in df['week_of_month']]\n",
    "df['event_name_1'] = df['event_name_1'].fillna(0)\n",
    "df['event_name_2'] = df['event_name_2'].fillna(0)\n",
    "df['event_type_1'] = df['event_type_1'].fillna(0)\n",
    "df['event_type_2'] = df['event_type_2'].fillna(0)\n",
    "df['is_week_end'] = [1 if i in [1,2] else 0 for i in df['wday']]\n",
    "df['close_to_week_days'] = [1 if i in [3,7] else 0 for i in df['wday']]\n",
    "df['mid_weeK_days'] = [1 if i in [4,5,6] else 0 for i in df['wday']]\n",
    "df['is_weekday'] = [1 if i not in [1,2] else 0 for i in df['wday']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['date','wm_yr_wk','weekday'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('best_model/data_level1.pkl')\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('best_model/data_level1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder(values,category_name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(values)\n",
    "    output = le.transform(values)\n",
    "    joblib.dump(le,f'best_model/preprocessed/{category_name}.pkl')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['item_id'] = label_encoder(df['item_id'].values,'item_id_le')\n",
    "df['dept_id'] = label_encoder(df['dept_id'].values,'dept_id_le')\n",
    "df['cat_id'] = label_encoder(df['cat_id'].values,'cat_id_le')\n",
    "df['state_id'] = label_encoder(df['state_id'].values,'state_id_le')\n",
    "df['store_id'] = label_encoder(df['store_id'].values,'store_id_le')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 2748.175506591797  MB\n",
      "********************** MEMORY USAGE AFTER COMPLETION **********************\n",
      "Memory usage is:  801.5511894226074  MB\n",
      "This is  29.166666666666668 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "cat_df = reduce_mem_usage(df[['item_id','dept_id','cat_id','state_id','store_id']])\n",
    "df = df.drop(['item_id','dept_id','cat_id','state_id','store_id'],axis=1)\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60034810, 24)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 2290.146255493164  MB\n",
      "********************** MEMORY USAGE AFTER COMPLETION **********************\n",
      "Memory usage is:  687.0438766479492  MB\n",
      "This is  30.0 % of the initial size\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['event_name_1'] = label_encoder(df['event_name_1'].astype(str).values,'event_name_1_le')\n",
    "df['event_name_2'] = label_encoder(df['event_name_2'].astype(str).values,'event_name_2_le')\n",
    "df['event_type_1'] = label_encoder(df['event_type_1'].astype(str).values,'event_type_1_le')\n",
    "df['event_type_2'] = label_encoder(df['event_type_2'].astype(str).values,'event_type_2_le')\n",
    "event_df = reduce_mem_usage(df[['event_name_1','event_name_2','event_type_1','event_type_2']])\n",
    "df = df.drop(['event_name_1','event_name_2','event_type_1','event_type_2'],axis=1)\n",
    "df_test = pd.concat([cat_df[0],event_df[0]],axis=1)\n",
    "df = pd.concat([df,df_test],axis=1)\n",
    "del df_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['units_sold'] = df['units_sold'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['store_item_sold_avg']  = df.groupby(['store_id','item_id'])['units_sold'].transform('mean').astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_store_item_cols = ['store_id','item_id','store_item_sold_avg']\n",
    "avg_store_item_df = df[avg_store_item_cols]\n",
    "avg_store_item_df.to_pickle(f'best_model/preprocessed/avg_store_item_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_store_item_df = pd.read_pickle('best_model/preprocessed/avg_store_item_df.pkl')\n",
    "avg_store_item_df = avg_store_item_df.drop_duplicates()\n",
    "avg_store_item_df.to_pickle(f'best_model/preprocessed/avg_store_item_df_unique.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_info = {'median_sell_price':str(df['sell_price'].median())}\n",
    "with open('best_model/preprocessed/other_info.json','w') as outfile:\n",
    "    json.dump(other_info,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'median_sell_price': '3.47'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
