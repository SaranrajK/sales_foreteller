{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style = 'white')\n",
    "# sns.set(style='whitegrid', color_codes=True)\n",
    "from tqdm import tqdm\n",
    "import plotly\n",
    "import time\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "import warnings\n",
    "import random\n",
    "import gc\n",
    "# import dask.dataframe as dd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import validation\n",
    "import re\n",
    "from sys import getsizeof\n",
    "import scipy\n",
    "from lightgbm import LGBMRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from pandas import Grouper\n",
    "from scipy.sparse import csr_matrix\n",
    "import joblib\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgbm\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', None)\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation,Dense,Flatten,BatchNormalization,Dropout,LeakyReLU,LSTM,Conv1D,TimeDistributed,MaxPool1D,Flatten,Input\n",
    "from tensorflow.keras.regularizers import l1,l2,l1_l2\n",
    "import tensorflow.keras.initializers as initializer\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import json\n",
    "tf.random.set_seed(120)\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "# rfered this blog\n",
    "# https://pythonhosted.org/calmap/\n",
    "# np.random.seed(sum(map(ord, 'calmap')))\n",
    "import calmap\n",
    "import pickle\n",
    "import joblib\n",
    "# from joblib import Parallel,delayed\n",
    "from scipy.stats import norm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "color_cycle = cycle([\"blue\",\"green\",\"red\",\"cyan\",\"magenta\",'yellow','black','darkorange','grey','lime'])\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "  incoming_df_mem_usage = df.memory_usage().sum() / 1024**2\n",
    "#   print(\"Memory usage of properties dataframe is :\",incoming_df_mem_usage,\" MB\")\n",
    "  na_list = [] # to keep track the columns that have missing values filled in\n",
    "  for col in df.columns:\n",
    "    if df[col].dtype != object:\n",
    "      is_int = False\n",
    "      max =  df[col].max()\n",
    "      min = df[col].min()\n",
    "\n",
    "      # integer doesn't support nan, so it needs to be filled\n",
    "      if not np.isfinite(df[col].all()):\n",
    "        na_list.app(col)\n",
    "        df[col].fillna(mn-1,inplace=True)\n",
    "      # test if column can be converted to integer\n",
    "      asint = df[col].fillna(0).astype(np.int64)\n",
    "      result = (df[col] - asint)\n",
    "      result = result.sum()\n",
    "\n",
    "      if result > -0.01 and result < 0.01:\n",
    "        is_int = True\n",
    "      if is_int:\n",
    "        if min>=0:\n",
    "          if max < 255:\n",
    "            df[col] = df[col].astype(np.uint8)\n",
    "          elif max < 65535:\n",
    "            df[col] = df[col].astype(np.uint16)\n",
    "          elif max < 4294967295:\n",
    "            df[col] = df[col].astype(np.uint32)\n",
    "          else:\n",
    "            df[col] = df[col].astype(np.uint64)\n",
    "        else:\n",
    "          if min > np.iinfo(np.int8).min and max < np.iinfo(np.int8).max:\n",
    "            df[col] = df[col].astype(np.int8)\n",
    "          elif min > np.iinfo(np.int16).min and max < np.iinfo(np.int16).max:\n",
    "            df[col] = df[col].astype(np.int16)\n",
    "          elif min > np.iinfo(np.int32).min and max < np.iinfo(np.int32).max:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "          elif min > np.iinfo(np.int64).min and max < np.iinfo(np.int64).max:\n",
    "            df[col] = df[col].astype(np.int64)\n",
    "      else:\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "         \n",
    "#   print(\"********************** MEMORY USAGE AFTER COMPLETION **********************\")\n",
    "  outgoing_df_mem_usage = df.memory_usage().sum() / 1024**2 \n",
    "#   print(\"Memory usage is: \",outgoing_df_mem_usage,\" MB\")\n",
    "#   print(\"This is \",100*outgoing_df_mem_usage/incoming_df_mem_usage,\"% of the initial size\")\n",
    "  return df, na_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have data for 1941 days, first 1913 for train dataset and 1914-1941 is validation dataset and we have to predict the sales for next 28 days so, we have add d_42 to d_1970 with values and make it as our test dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the below instructions before giving the input data to predict.**\n",
    "1. You have sales data information from 0 to 1942 and we have calender(for example event information and snap data information)\n",
    "   and we have sell price information(price of a product over time) till 1969, so we have predict till 1969 days only\n",
    "2. from the user input perspective the function need two inputs one is for what products that user want to see the predicted\n",
    "   output and for what are the days user can give range of days or specific days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function1(input_data_points,pred_day):\n",
    "    pred_day.sort()\n",
    "    raw_input_data = {'id':input_data_points,'item_id':[],'dept_id':[],'cat_id':[],'store_id':[],'state_id':[]}\n",
    "    for data_point in input_data_points:\n",
    "        data_point = data_point.split('_')\n",
    "        raw_input_data['item_id'].append(f'{data_point[0]}_{data_point[1]}_{data_point[2]}')\n",
    "        raw_input_data['dept_id'].append(f'{data_point[0]}_{data_point[1]}')\n",
    "        raw_input_data['cat_id'].append(f'{data_point[0]}')\n",
    "        raw_input_data['store_id'].append(f'{data_point[3]}_{data_point[4]}')\n",
    "        raw_input_data['state_id'].append(f'{data_point[3]}')\n",
    "        \n",
    "        \n",
    "    sales_train_evaluation = pd.read_pickle(r\"best_model/sales_train_evaluation.pkl\")  #The properties dataset\n",
    "    calendar = pd.read_pickle(r\"best_model/calender.pkl\")  #The properties dataset\n",
    "    sell_prices = pd.read_pickle(r\"best_model/sell_prices.pkl\")  #The properties dataset\n",
    "\n",
    "    if(len(pred_day)!=1):\n",
    "        start_day,end_day = min(pred_day),max(pred_day)\n",
    "    else:\n",
    "        start_day,end_day = pred_day[0],pred_day[0]\n",
    "\n",
    "    sales_train_evaluation_ip = pd.DataFrame.from_dict(raw_input_data)\n",
    "\n",
    "    sales_train_evaluation_col = [column for column in sales_train_evaluation.columns if 'd_' not in column]\n",
    "\n",
    "    day_range_to_predict = [f'd_{day}' for day in range(start_day,end_day+1)]\n",
    "\n",
    "    test_day = [f'd_{day}' for day in range(1942,1970)]\n",
    "\n",
    "    d_cols = []\n",
    "    for d_num in range(start_day - 90,start_day):\n",
    "        d_cols.append(f'd_{d_num}')\n",
    "    d_cols = d_cols + day_range_to_predict\n",
    "    days_from_test_data = list(set(d_cols).intersection(test_day))\n",
    "\n",
    "    for d in days_from_test_data:\n",
    "        sales_train_evaluation_ip[d] = 0\n",
    "        sales_train_evaluation_ip[d] = sales_train_evaluation_ip[d].astype(np.int16)\n",
    "\n",
    "    sales_train_evaluation_ip_cols_filled = [d for d in sales_train_evaluation_ip.columns if 'd_' in d]\n",
    "    days_from_rest = set(sales_train_evaluation_ip_cols_filled) ^ set(d_cols)\n",
    "    items_to_select = sales_train_evaluation_ip[['id']]\n",
    "    sales_train_selected_item = pd.merge(sales_train_evaluation,items_to_select,on='id',how='inner')\n",
    "    sales_train_evaluation_d_columns = sales_train_selected_item[days_from_rest]\n",
    "    sales_train_evaluation_ip = pd.concat([sales_train_evaluation_ip,sales_train_evaluation_d_columns],axis=1)\n",
    "    sales_train_evaluation_ip_cols_cat,sales_train_evaluation_ip_cols_d = sales_train_evaluation_col,[d for d in list(sales_train_evaluation_ip.columns) if 'd_' in d]\n",
    "    sales_train_evaluation_ip = sales_train_evaluation_ip[sales_train_evaluation_ip_cols_cat + list(np.sort(np.asarray(sales_train_evaluation_ip_cols_d)))]\n",
    "    \n",
    "    day_cols_calender = [column for column in sales_train_evaluation_ip.columns if 'd_' in column]\n",
    "    calendar_ip = calendar[calendar['d'].isin(day_cols_calender)]\n",
    "    sell_prices_ip = sell_prices\n",
    "    \n",
    "    \n",
    "    df_ip = pd.melt(sales_train_evaluation_ip, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='units_sold').dropna()\n",
    "    df_ip = pd.merge(df_ip, calendar_ip, on='d', how='left')\n",
    "    df_ip = pd.merge(df_ip, sell_prices_ip, on=['store_id','item_id','wm_yr_wk'], how='left') \n",
    "    \n",
    "    df_ip['date'] = pd.to_datetime(df_ip['date'])\n",
    "    df_ip['day_of_month'] = df_ip['date'].dt.day.astype('int8')\n",
    "    df_ip['week_of_month'] = df_ip['date'].apply(lambda d: (d.day-1) // 7 + 1).astype('int8')\n",
    "    df_ip['week_of_year'] = df_ip['date'].dt.week.astype('int8')\n",
    "    df_ip['quarter_of_year'] = df_ip['date'].dt.quarter.astype('int8')\n",
    "    \n",
    "    df_ip['is_month_start'] = [1 if i == 1 else 0 for i in df_ip['week_of_month']]\n",
    "    df_ip['is_month_end'] = [1 if i in [4,5] else 0 for i in df_ip['week_of_month']]\n",
    "    df_ip['event_name_1'] = df_ip['event_name_1'].fillna(0)\n",
    "    df_ip['event_name_2'] = df_ip['event_name_2'].fillna(0)\n",
    "    df_ip['event_type_1'] = df_ip['event_type_1'].fillna(0)\n",
    "    df_ip['event_type_2'] = df_ip['event_type_2'].fillna(0)\n",
    "    df_ip['is_week_end'] = [1 if i in [1,2] else 0 for i in df_ip['wday']]\n",
    "    df_ip['close_to_week_days'] = [1 if i in [3,7] else 0 for i in df_ip['wday']]\n",
    "    df_ip['mid_weeK_days'] = [1 if i in [4,5,6] else 0 for i in df_ip['wday']]\n",
    "    df_ip['is_weekday'] = [1 if i not in [1,2] else 0 for i in df_ip['wday']]\n",
    "    \n",
    "    df_ip = df_ip.drop(['date','wm_yr_wk','weekday'],axis=1)\n",
    "\n",
    "    avg_store_item_df = pd.read_pickle('best_model/preprocessed/avg_store_item_df_unique.pkl')\n",
    "    avg_store_item_df = avg_store_item_df.drop_duplicates()\n",
    "    \n",
    "    df_ip = pd.merge(df_ip,avg_store_item_df,left_on=['store_id','item_id'],right_on=['store_id','item_id'])\n",
    "    \n",
    "    item_id_le = joblib.load('best_model/preprocessed/item_id_le.pkl')\n",
    "    dept_id_le = joblib.load('best_model/preprocessed/dept_id_le.pkl')\n",
    "    cat_id_le = joblib.load('best_model/preprocessed/cat_id_le.pkl')\n",
    "    state_id_le = joblib.load('best_model/preprocessed/state_id_le.pkl')\n",
    "    store_id_le = joblib.load('best_model/preprocessed/store_id_le.pkl')\n",
    "    event_name_1_le = joblib.load('best_model/preprocessed/event_name_1_le.pkl')\n",
    "    event_name_2_le = joblib.load('best_model/preprocessed/event_name_2_le.pkl')\n",
    "    event_type_1_le = joblib.load('best_model/preprocessed/event_type_1_le.pkl')\n",
    "    event_type_2_le = joblib.load('best_model/preprocessed/event_type_2_le.pkl')\n",
    "\n",
    "    df_ip['item_id'] = item_id_le.transform(df_ip['item_id'].values)\n",
    "    df_ip['dept_id'] = dept_id_le.transform(df_ip['dept_id'].values)\n",
    "    df_ip['cat_id'] = cat_id_le.transform(df_ip['cat_id'].values)\n",
    "    df_ip['state_id'] = state_id_le.transform(df_ip['state_id'].values)\n",
    "    df_ip['store_id'] = store_id_le.transform(df_ip['store_id'].values)\n",
    "\n",
    "    cat_df_ip = reduce_mem_usage(df_ip[['item_id','dept_id','cat_id','state_id','store_id']])\n",
    "    df_ip = df_ip.drop(['item_id','dept_id','cat_id','state_id','store_id'],axis=1)\n",
    "\n",
    "    df_ip['event_name_1'] = event_name_1_le.transform(df_ip['event_name_1'].astype(str).values)\n",
    "    df_ip['event_name_2'] = event_name_2_le.transform(df_ip['event_name_2'].astype(str).values)\n",
    "    df_ip['event_type_1'] = event_type_1_le.transform(df_ip['event_type_1'].astype(str).values)\n",
    "    df_ip['event_type_2'] = event_type_2_le.transform(df_ip['event_type_2'].astype(str).values)\n",
    "\n",
    "    event_df_ip = reduce_mem_usage(df_ip[['event_name_1','event_name_2','event_type_1','event_type_2']])\n",
    "    df_ip = df_ip.drop(['event_name_1','event_name_2','event_type_1','event_type_2'],axis=1)\n",
    "    df_test_ip = pd.concat([cat_df_ip[0],event_df_ip[0]],axis=1) \n",
    "    df_ip = pd.concat([df_ip,df_test_ip],axis=1)\n",
    "    del cat_df_ip\n",
    "    del event_df_ip\n",
    "    del df_test_ip\n",
    "    gc.collect()\n",
    "    \n",
    "    for lag_day,lag_column in zip([7,28,29,30,31,90],[f\"lag_{lag}\" for lag in [7,28,29,30,31,90]]):\n",
    "        df_ip[lag_column] = df_ip.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['units_sold'].shift(lag_day).astype(np.float16)\n",
    "\n",
    "    for rolling_day,rolling_column in zip([7,28,29,30,31,90],[f\"rolling_{rolling}\" for rolling in [7,28,29,30,31,90]]):\n",
    "        df_ip[rolling_column] = df_ip.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['units_sold'].transform(lambda x: x.rolling(window=rolling_day).mean()).astype(np.float16)\n",
    "\n",
    "    for rolling_day,rolling_column in zip([7,28,29,30,31,90],[f\"rolling_std_{rolling}\" for rolling in [7,28,29,30,31,90]]):\n",
    "        df_ip[rolling_column] = df_ip.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['units_sold'].transform(lambda x: x.rolling(window=rolling_day).std()).astype(np.float16)\n",
    "\n",
    "    for expanding_day,expanding_column in zip([7,28,29,30,31,90],[f\"expanding_{expanding}\" for expanding in [7,28,29,30,31,90]]):\n",
    "        df_ip[expanding_column] = df_ip.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['units_sold'].transform(lambda x: x.expanding(expanding_day).mean()).astype(np.float16)\n",
    "\n",
    "    for expanding_day,expanding_column in zip([7,28,29,30,31,90],[f\"expanding_std_{expanding}\" for expanding in [7,28,29,30,31,90]]):\n",
    "        df_ip[expanding_column] = df_ip.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['units_sold'].transform(lambda x: x.expanding(expanding_day).std()).astype(np.float16)\n",
    "        \n",
    "    df_ip['d'] = df_ip['d'].apply(lambda x:x.split('_')[1]).astype(np.int16)\n",
    "    df_ip['store_id'] = [str(i.split('_')[-3])+'_'+str(i.split('_')[-2]) for i in df_ip['id']]\n",
    "    df_ip['units_sold'] = df_ip['units_sold'].fillna(0)\n",
    "    \n",
    "    with open('best_model/preprocessed/other_info.json') as f:\n",
    "        other_info = json.load(f)\n",
    "\n",
    "    df_ip['sell_price'] =  df_ip['sell_price'].fillna(other_info['median_sell_price'])\n",
    "    df_ip['daily_avg_sales_unit'] = df_ip.groupby(['id','d'])['units_sold'].transform('mean').astype(np.float16)\n",
    "    df_ip['weekly_avg_sales_unit'] = df_ip.groupby(['id','week_of_month'])['units_sold'].transform('mean').astype(np.float16)\n",
    "    df_ip['sales_usd'] = df_ip['sell_price'] * df_ip['units_sold']\n",
    "    \n",
    "    final_value = {}\n",
    "    final_value['valid_pred'] = {}\n",
    "    final_value['test_pred'] = {}\n",
    "    remove_columns = ['id','store_id','sales_usd','daily_avg_sales_unit','is_weekday','units_sold']\n",
    "    store_list = df_ip['store_id'].unique()\n",
    "    for store in store_list:\n",
    "        df_store = df_ip[df_ip['store_id'] == store]  \n",
    "        snap_feature  = ['snap_'+store.split('_')[0]]\n",
    "        selected_columns = [column for column in df_store.columns if '7' not in column and 'rolling' not in column and 'snap' not in column and 'state' not in column]\n",
    "        df_store = df_store[selected_columns+snap_feature]\n",
    "        print(df_store.shape)\n",
    "        x_test = df_store.loc[(df_store['d']>=pred_day[0])].drop(remove_columns,axis=1)\n",
    "        print('*****Prediction for Store: {}*****'.format(store))  \n",
    "        dummy_model = joblib.load(f'best_model/gbdt/{store}_median_model1.pkl')\n",
    "        test_pred_df = pd.DataFrame()\n",
    "        #     val_score = np.sqrt(metrics.mean_squared_error(val_pred,y_valid))\n",
    "        y_pred = dummy_model.predict(x_test)\n",
    "        y_pred = [round(val) for val in y_pred]\n",
    "        test_pred_df['id'] = df_store.loc[(df_store['d']>=pred_day[0])]['id']\n",
    "        test_pred_df['store_id'] = df_store.loc[(df_store['d']>=pred_day[0])]['d']\n",
    "        test_pred_df['units_sold'] = y_pred\n",
    "        final_value['test_pred'][store] = test_pred_df\n",
    "\n",
    "    dict_store_test = {}\n",
    "    dict_store_test['id'] = []\n",
    "    dict_store_test['d'] = []\n",
    "    dict_store_test['units_sold'] = []\n",
    "    for k,v in final_value['test_pred'].items():\n",
    "        for i in v.values:\n",
    "            dict_store_test['id'].append(i[0])\n",
    "            dict_store_test['d'].append(i[1])\n",
    "            dict_store_test['units_sold'].append(i[2])\n",
    "\n",
    "    test_pred = pd.DataFrame.from_dict(dict_store_test)\n",
    "    test_pred = pd.pivot(test_pred, index = 'id', columns = 'd', values = 'units_sold').reset_index()\n",
    "    test_pred.columns = ['id'] + ['d_' + str(i) for i in test_pred.columns if 'id' not in str(i)]\n",
    "    test_pred = test_pred[['id']+['d_'+str(i) for i in pred_day]]\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if we want test with multiple products / days uncomment and test the following code\n",
    "# start_time = time.time()\n",
    "# input_data_points = ['FOODS_3_821_WI_3_evaluation','FOODS_3_822_WI_3_evaluation','FOODS_3_823_WI_3_evaluation']\n",
    "# days_to_predict = [day for day in range(1913,1941)]\n",
    "# predicted_data = function1(input_data_points,days_to_predict)\n",
    "# print(\"running time {:.3f} secs...\".format(time.time() - start_time))\n",
    "# predicted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 45)\n",
      "*****Prediction for Store: WI_3*****\n",
      "running time 4.447 secs...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>d_1920</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS_3_821_WI_3_evaluation</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id  d_1920\n",
       "0  FOODS_3_821_WI_3_evaluation     1.0"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "input_data_points = ['FOODS_3_821_WI_3_evaluation']\n",
    "days_to_predict = [1920]\n",
    "predicted_data = function1(input_data_points,days_to_predict)\n",
    "print(\"running time {:.3f} secs...\".format(time.time() - start_time))\n",
    "predicted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_aggregator(inp_df):\n",
    "    roll_mat_csr_store = {}\n",
    "    stores_list = inp_df['store_id'].unique()\n",
    "    for store in stores_list:\n",
    "        store_df = inp_df[inp_df['store_id']==store]\n",
    "        dummies_list = [store_df.state_id, store_df.store_id, \n",
    "                    store_df.cat_id, store_df.dept_id, \n",
    "                    store_df.state_id +'_'+ store_df.cat_id, store_df.state_id +'_'+ store_df.dept_id,\n",
    "                    store_df.store_id +'_'+ store_df.cat_id, store_df.store_id +'_'+ store_df.dept_id, \n",
    "                    store_df.item_id, store_df.state_id +'_'+ store_df.item_id, store_df.id]\n",
    "\n",
    "        # Level_0 aggregation of all values:\n",
    "        dummy_df =[pd.DataFrame(np.ones(store_df.shape[0]).astype(np.int8), \n",
    "                                       index=store_df.index, columns=['all']).T]\n",
    "        # List of dummy dataframes:\n",
    "        for i, cats in enumerate(dummies_list):\n",
    "            dummy_df +=[pd.get_dummies(cats, drop_first=False, dtype=np.int8).T]\n",
    "\n",
    "        # Concat dummy dataframes in one go:\n",
    "        roll_mat_df = pd.concat(dummy_df, keys=list(range(12)), \n",
    "                                names=['level','id'])#.astype(np.int8, copy=False)\n",
    "\n",
    "        roll_index = roll_mat_df.index\n",
    "        roll_mat_csr = csr_matrix(roll_mat_df.values)\n",
    "        roll_mat_csr_store[store] = roll_mat_csr\n",
    "    return roll_mat_csr_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucntion to calculate S weights for each store:\n",
    "def get_s(df_ip,day_range,roll_mat_csr_store,drop_days=0):\n",
    "    weight_s_list = {}\n",
    "    stores_list = df_ip['store_id'].unique()\n",
    "    for store in stores_list:    \n",
    "        \"\"\"\n",
    "        drop_days: int, equals 0 by default, so S is calculated on all data.\n",
    "                   If equals 28, last 28 days won't be used in calculating S.\n",
    "        \"\"\"\n",
    "        data_store = df_ip[df_ip['store_id']==store]\n",
    "        roll_mat_csr = roll_mat_csr_store[store]\n",
    "        # Rollup sales:\n",
    "#         d_name = ['d_' + str(i) for i in range((day_range[0]-90),day_range[1]+1)]\n",
    "#         print(d_name)\n",
    "#         print(roll_mat_csr.shape,data_store[d_name].values.shape)\n",
    "        d_name = [d for d in df_ip.columns if 'd_' in d]\n",
    "        sales_train_val = roll_mat_csr * data_store[d_name].values\n",
    "\n",
    "        no_sales = np.cumsum(sales_train_val, axis=1) == 0\n",
    "        sales_train_val = np.where(no_sales, np.nan, sales_train_val)\n",
    "\n",
    "        # Denominator of RMSSE \n",
    "        weight1 = np.nanmean(np.diff(sales_train_val,axis=1)**2,axis=1)\n",
    "        weight_s_list[store] = weight1\n",
    "    return weight_s_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functinon to calculate weights:\n",
    "def get_w(sale_usd,roll_mat_csr_store,store):\n",
    "    # calulate sales in USD for each id:\n",
    "    total_sales_usd = sale_usd.groupby(\n",
    "        ['id'], sort=False)['sales_usd'].apply(np.sum).values\n",
    "    \n",
    "    roll_mat_csr = roll_mat_csr_store[store]\n",
    "    # Roll up total sales by ids to higher levels:\n",
    "    weight2 = roll_mat_csr * total_sales_usd\n",
    "    return 12*weight2/np.sum(weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since WRMSSE calucated for each stores so we have 3049 rows and 9180 time series\n",
    "# Function to do quick rollups:\n",
    "store_name = ''\n",
    "def rollup(v,store,roll_mat_csr_store):\n",
    "    '''\n",
    "    v - np.array of size (3049 rows, n day columns)\n",
    "    v_rolledup - array of size (n, 9180)\n",
    "    '''\n",
    "    roll_mat_csr = roll_mat_csr_store[store]\n",
    "    return roll_mat_csr*v #(v.T*roll_mat_csr.T).T\n",
    "\n",
    "\n",
    "# Function to calculate WRMSSE:\n",
    "key = 0\n",
    "def wrmsse(preds, y_true,store_name,roll_mat_csr_store,SW_store,num_of_items,num_of_days):\n",
    "    '''\n",
    "    preds - Predictions: pd.DataFrame of size (3049 rows, N day columns)\n",
    "    y_true - True values: pd.DataFrame of size (3049 rows, N day columns)\n",
    "    sequence_length - np.array of size (9180,)\n",
    "    sales_weight - sales weights based on last 28 days: np.array (9180,)\n",
    "    '''\n",
    "    preds = preds[-(num_of_items * num_of_days):]\n",
    "    y_true = y_true[-(num_of_items * num_of_days):]\n",
    "    preds = preds.reshape(num_of_days, num_of_items).T\n",
    "    y_true = y_true.reshape(num_of_days, num_of_items).T  \n",
    "    final_wrmsse = np.sqrt(np.mean(np.square(rollup(preds-y_true,store_name,roll_mat_csr_store)),axis=1)) * SW_store\n",
    "#     final_wrmsse = [round(val,4) for val in final_wrmsse]\n",
    "    final_wrmsse = np.nan_to_num(final_wrmsse)\n",
    "    final_wrmsse = np.asarray([round(val,4) for val in final_wrmsse])\n",
    "#     print(np.sum(final_wrmsse))\n",
    "    return 'wrmsse', round(np.sum(final_wrmsse)/12,4),False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function2(input_data_points,pred_day):\n",
    "    test_range = all(ele <= 1941 for ele in pred_day) \n",
    "    if test_range:\n",
    "        pred_day.sort()    \n",
    "        raw_input_data = {'id':input_data_points,'item_id':[],'dept_id':[],'cat_id':[],'store_id':[],'state_id':[]}\n",
    "        for data_point in input_data_points:\n",
    "            data_point = data_point.split('_')\n",
    "            raw_input_data['item_id'].append(f'{data_point[0]}_{data_point[1]}_{data_point[2]}')\n",
    "            raw_input_data['dept_id'].append(f'{data_point[0]}_{data_point[1]}')\n",
    "            raw_input_data['cat_id'].append(f'{data_point[0]}')\n",
    "            raw_input_data['store_id'].append(f'{data_point[3]}_{data_point[4]}')\n",
    "            raw_input_data['state_id'].append(f'{data_point[3]}')\n",
    "\n",
    "\n",
    "        sales_train_evaluation = pd.read_pickle(r\"best_model/sales_train_evaluation.pkl\")  #The properties dataset\n",
    "        calendar = pd.read_pickle(r\"best_model/calender.pkl\")  #The properties dataset\n",
    "        sell_prices = pd.read_pickle(r\"best_model/sell_prices.pkl\")  #The properties dataset\n",
    "\n",
    "        if(len(days_to_predict)!=1):\n",
    "            start_day,end_day = min(days_to_predict),max(days_to_predict)\n",
    "        else:\n",
    "            start_day,end_day = days_to_predict[0],days_to_predict[0]\n",
    "\n",
    "        sales_train_evaluation_ip = pd.DataFrame.from_dict(raw_input_data)\n",
    "\n",
    "        sales_train_evaluation_col = [column for column in sales_train_evaluation.columns if 'd_' not in column]\n",
    "\n",
    "        day_range_to_predict = [f'd_{day}' for day in range(start_day,end_day+1)]\n",
    "\n",
    "        test_day = [f'd_{day}' for day in range(1942,1970)]\n",
    "\n",
    "        d_cols = []\n",
    "        for d_num in range(start_day - 90,start_day):\n",
    "            d_cols.append(f'd_{d_num}')\n",
    "        d_cols = d_cols + day_range_to_predict\n",
    "        days_from_test_data = list(set(d_cols).intersection(test_day))\n",
    "\n",
    "        for d in days_from_test_data:\n",
    "            sales_train_evaluation_ip[d] = 0\n",
    "            sales_train_evaluation_ip[d] = sales_train_evaluation_ip[d].astype(np.int16)\n",
    "\n",
    "        sales_train_evaluation_ip_cols_filled = [d for d in sales_train_evaluation_ip.columns if 'd_' in d]\n",
    "        days_from_rest = set(sales_train_evaluation_ip_cols_filled) ^ set(d_cols)\n",
    "        items_to_select = sales_train_evaluation_ip[['id']]\n",
    "        sales_train_selected_item = pd.merge(sales_train_evaluation,items_to_select,on='id',how='inner')\n",
    "        sales_train_evaluation_d_columns = sales_train_selected_item[days_from_rest]\n",
    "        sales_train_evaluation_ip = pd.concat([sales_train_evaluation_ip,sales_train_evaluation_d_columns],axis=1)\n",
    "        sales_train_evaluation_ip_cols_cat,sales_train_evaluation_ip_cols_d = sales_train_evaluation_col,[d for d in list(sales_train_evaluation_ip.columns) if 'd_' in d]\n",
    "        sales_train_evaluation_ip = sales_train_evaluation_ip[sales_train_evaluation_ip_cols_cat + list(np.sort(np.asarray(sales_train_evaluation_ip_cols_d)))]\n",
    "\n",
    "        day_cols_calender = [column for column in sales_train_evaluation_ip.columns if 'd_' in column]\n",
    "        calendar_ip = calendar[calendar['d'].isin(day_cols_calender)]\n",
    "        sell_prices_ip = sell_prices\n",
    "\n",
    "        roll_mat_csr_store = level_aggregator(sales_train_evaluation_ip)\n",
    "        S = get_s(sales_train_evaluation_ip,pred_day,roll_mat_csr_store,drop_days=0)  \n",
    "        stores_list = sales_train_evaluation_ip['store_id'].unique()\n",
    "\n",
    "        df_ip = pd.melt(sales_train_evaluation_ip, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='units_sold').dropna()\n",
    "        df_ip = pd.merge(df_ip, calendar_ip, on='d', how='left')\n",
    "        df_ip = pd.merge(df_ip, sell_prices_ip, on=['store_id','item_id','wm_yr_wk'], how='left') \n",
    "\n",
    "        df_ip['date'] = pd.to_datetime(df_ip['date'])\n",
    "        df_ip['day_of_month'] = df_ip['date'].dt.day.astype('int8')\n",
    "        df_ip['week_of_month'] = df_ip['date'].apply(lambda d: (d.day-1) // 7 + 1).astype('int8')\n",
    "        df_ip['week_of_year'] = df_ip['date'].dt.week.astype('int8')\n",
    "        df_ip['quarter_of_year'] = df_ip['date'].dt.quarter.astype('int8')\n",
    "\n",
    "        df_ip['is_month_start'] = [1 if i == 1 else 0 for i in df_ip['week_of_month']]\n",
    "        df_ip['is_month_end'] = [1 if i in [4,5] else 0 for i in df_ip['week_of_month']]\n",
    "        df_ip['event_name_1'] = df_ip['event_name_1'].fillna(0)\n",
    "        df_ip['event_name_2'] = df_ip['event_name_2'].fillna(0)\n",
    "        df_ip['event_type_1'] = df_ip['event_type_1'].fillna(0)\n",
    "        df_ip['event_type_2'] = df_ip['event_type_2'].fillna(0)\n",
    "        df_ip['is_week_end'] = [1 if i in [1,2] else 0 for i in df_ip['wday']]\n",
    "        df_ip['close_to_week_days'] = [1 if i in [3,7] else 0 for i in df_ip['wday']]\n",
    "        df_ip['mid_weeK_days'] = [1 if i in [4,5,6] else 0 for i in df_ip['wday']]\n",
    "        df_ip['is_weekday'] = [1 if i not in [1,2] else 0 for i in df_ip['wday']]\n",
    "\n",
    "        df_ip = df_ip.drop(['date','wm_yr_wk','weekday'],axis=1)\n",
    "\n",
    "        avg_store_item_df = pd.read_pickle('best_model/preprocessed/avg_store_item_df_unique.pkl')\n",
    "        avg_store_item_df = avg_store_item_df.drop_duplicates()\n",
    "\n",
    "        df_ip = pd.merge(df_ip,avg_store_item_df,left_on=['store_id','item_id'],right_on=['store_id','item_id'])\n",
    "\n",
    "        item_id_le = joblib.load('best_model/preprocessed/item_id_le.pkl')\n",
    "        dept_id_le = joblib.load('best_model/preprocessed/dept_id_le.pkl')\n",
    "        cat_id_le = joblib.load('best_model/preprocessed/cat_id_le.pkl')\n",
    "        state_id_le = joblib.load('best_model/preprocessed/state_id_le.pkl')\n",
    "        store_id_le = joblib.load('best_model/preprocessed/store_id_le.pkl')\n",
    "        event_name_1_le = joblib.load('best_model/preprocessed/event_name_1_le.pkl')\n",
    "        event_name_2_le = joblib.load('best_model/preprocessed/event_name_2_le.pkl')\n",
    "        event_type_1_le = joblib.load('best_model/preprocessed/event_type_1_le.pkl')\n",
    "        event_type_2_le = joblib.load('best_model/preprocessed/event_type_2_le.pkl')\n",
    "\n",
    "        df_ip['item_id'] = item_id_le.transform(df_ip['item_id'].values)\n",
    "        df_ip['dept_id'] = dept_id_le.transform(df_ip['dept_id'].values)\n",
    "        df_ip['cat_id'] = cat_id_le.transform(df_ip['cat_id'].values)\n",
    "        df_ip['state_id'] = state_id_le.transform(df_ip['state_id'].values)\n",
    "        df_ip['store_id'] = store_id_le.transform(df_ip['store_id'].values)\n",
    "\n",
    "        cat_df_ip = reduce_mem_usage(df_ip[['item_id','dept_id','cat_id','state_id','store_id']])\n",
    "        df_ip = df_ip.drop(['item_id','dept_id','cat_id','state_id','store_id'],axis=1)\n",
    "\n",
    "        df_ip['event_name_1'] = event_name_1_le.transform(df_ip['event_name_1'].astype(str).values)\n",
    "        df_ip['event_name_2'] = event_name_2_le.transform(df_ip['event_name_2'].astype(str).values)\n",
    "        df_ip['event_type_1'] = event_type_1_le.transform(df_ip['event_type_1'].astype(str).values)\n",
    "        df_ip['event_type_2'] = event_type_2_le.transform(df_ip['event_type_2'].astype(str).values)\n",
    "\n",
    "        event_df_ip = reduce_mem_usage(df_ip[['event_name_1','event_name_2','event_type_1','event_type_2']])\n",
    "        df_ip = df_ip.drop(['event_name_1','event_name_2','event_type_1','event_type_2'],axis=1)\n",
    "        df_test_ip = pd.concat([cat_df_ip[0],event_df_ip[0]],axis=1) \n",
    "        df_ip = pd.concat([df_ip,df_test_ip],axis=1)\n",
    "        del cat_df_ip\n",
    "        del event_df_ip\n",
    "        del df_test_ip\n",
    "        gc.collect()\n",
    "\n",
    "        for lag_day,lag_column in zip([7,28,29,30,31,90],[f\"lag_{lag}\" for lag in [7,28,29,30,31,90]]):\n",
    "            df_ip[lag_column] = df_ip.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['units_sold'].shift(lag_day).astype(np.float16)\n",
    "\n",
    "        for rolling_day,rolling_column in zip([7,28,29,30,31,90],[f\"rolling_{rolling}\" for rolling in [7,28,29,30,31,90]]):\n",
    "            df_ip[rolling_column] = df_ip.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['units_sold'].transform(lambda x: x.rolling(window=rolling_day).mean()).astype(np.float16)\n",
    "\n",
    "        for rolling_day,rolling_column in zip([7,28,29,30,31,90],[f\"rolling_std_{rolling}\" for rolling in [7,28,29,30,31,90]]):\n",
    "            df_ip[rolling_column] = df_ip.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['units_sold'].transform(lambda x: x.rolling(window=rolling_day).std()).astype(np.float16)\n",
    "\n",
    "        for expanding_day,expanding_column in zip([7,28,29,30,31,90],[f\"expanding_{expanding}\" for expanding in [7,28,29,30,31,90]]):\n",
    "            df_ip[expanding_column] = df_ip.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['units_sold'].transform(lambda x: x.expanding(expanding_day).mean()).astype(np.float16)\n",
    "\n",
    "        for expanding_day,expanding_column in zip([7,28,29,30,31,90],[f\"expanding_std_{expanding}\" for expanding in [7,28,29,30,31,90]]):\n",
    "            df_ip[expanding_column] = df_ip.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['units_sold'].transform(lambda x: x.expanding(expanding_day).std()).astype(np.float16)\n",
    "\n",
    "        df_ip['d'] = df_ip['d'].apply(lambda x:x.split('_')[1]).astype(np.int16)\n",
    "        df_ip['store_id'] = [str(i.split('_')[-3])+'_'+str(i.split('_')[-2]) for i in df_ip['id']]\n",
    "        df_ip['units_sold'] = df_ip['units_sold'].fillna(0)\n",
    "\n",
    "        with open('best_model/preprocessed/other_info.json') as f:\n",
    "            other_info = json.load(f)\n",
    "\n",
    "        df_ip['sell_price'] =  df_ip['sell_price'].fillna(float(other_info['median_sell_price']))\n",
    "        df_ip['daily_avg_sales_unit'] = df_ip.groupby(['id','d'])['units_sold'].transform('mean').astype(np.float16)\n",
    "        df_ip['weekly_avg_sales_unit'] = df_ip.groupby(['id','week_of_month'])['units_sold'].transform('mean').astype(np.float16)\n",
    "        df_ip['sales_usd'] = df_ip['sell_price'] * df_ip['units_sold']\n",
    "\n",
    "        w_weights_list = {}\n",
    "        for store in stores_list:\n",
    "            data = df_ip[df_ip['store_id'] == store]\n",
    "            data = data[data['d']>=pred_day[0]-90]\n",
    "            W = get_w(data[['id','sales_usd']],roll_mat_csr_store,store)\n",
    "            w_weights_list[store] = W\n",
    "            del data\n",
    "            gc.collect()\n",
    "\n",
    "        SW_store = {}\n",
    "        for key in w_weights_list.keys():\n",
    "            SW_store[key] = w_weights_list[key]/np.sqrt(S[key])\n",
    "\n",
    "        SW_store1 = SW_store\n",
    "\n",
    "        final_value = {}\n",
    "        final_value['valid_pred'] = {}\n",
    "        final_value['test_pred'] = {}\n",
    "        remove_columns = ['id','store_id','sales_usd','daily_avg_sales_unit','is_weekday','units_sold']\n",
    "        store_list = df_ip['store_id'].unique()\n",
    "        for store in store_list:\n",
    "    #         print(f\"predicting for the store {store}\")\n",
    "            SW_store = SW_store1[store]\n",
    "            df_store = df_ip[df_ip['store_id'] == store] \n",
    "            df_store = df_store[df_store['d'].isin(pred_day)]\n",
    "            snap_feature  = ['snap_'+store.split('_')[0]]\n",
    "            selected_columns = [column for column in df_store.columns if '7' not in column and 'rolling' not in column and 'snap' not in column and 'state' not in column]\n",
    "            df_store = df_store[selected_columns+snap_feature]\n",
    "            x_test,y_true = df_store.loc[(df_store['d']>=pred_day[0])].drop(remove_columns,axis=1),df_store[df_store['d']>=pred_day[0]]['units_sold'].values\n",
    "            print('*****Prediction for Store: {}*****'.format(store))  \n",
    "            dummy_model = joblib.load(f'best_model/gbdt/{store}_median_model1.pkl')\n",
    "            test_pred_df = pd.DataFrame()\n",
    "            #     val_score = np.sqrt(metrics.mean_squared_error(val_pred,y_valid))\n",
    "            y_pred = dummy_model.predict(x_test)\n",
    "            y_pred = np.asarray([round(val) for val in y_pred])\n",
    "            num_of_days = len(pred_day)\n",
    "            num_of_items = len(x_test['item_id'].unique())\n",
    "            _,wrmsse_score,_ = wrmsse(y_pred,y_true,store,roll_mat_csr_store,SW_store,num_of_items,num_of_days)\n",
    "            test_pred_df['id'] = df_store.loc[(df_store['d']>=pred_day[0])]['id']\n",
    "            test_pred_df['store_id'] = df_store.loc[(df_store['d']>=pred_day[0])]['d']\n",
    "            test_pred_df['units_sold'] = y_pred\n",
    "            final_value['test_pred'][store] = test_pred_df\n",
    "\n",
    "        dict_store_test = {}\n",
    "        dict_store_test['id'] = []\n",
    "        dict_store_test['d'] = []\n",
    "        dict_store_test['units_sold'] = []\n",
    "        for k,v in final_value['test_pred'].items():\n",
    "            for i in v.values:\n",
    "                dict_store_test['id'].append(i[0])\n",
    "                dict_store_test['d'].append(i[1])\n",
    "                dict_store_test['units_sold'].append(i[2]) \n",
    "        test_pred = pd.DataFrame.from_dict(dict_store_test)\n",
    "    #     test_pred = pd.pivot(test_pred, index = 'id', columns = 'd', values = 'units_sold').reset_index()\n",
    "        return wrmsse_score\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if we want test with multiple products / days uncomment and test the following code\n",
    "# start_time = time.time()\n",
    "# input_data_points = ['FOODS_3_821_WI_3_evaluation','FOODS_3_822_WI_3_evaluation','FOODS_3_823_WI_3_evaluation']\n",
    "# days_to_predict = [day for day in range(1913,1941)]\n",
    "# wrmsse_score = function2(input_data_points,days_to_predict)\n",
    "# print(\"running time {:.3f} secs...\".format(time.time() - start_time))\n",
    "# wrmsse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Prediction for Store: WI_3*****\n",
      "WRMSSE score is 0.6061\n",
      "running time 5.951 secs...\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "input_data_points = ['FOODS_3_821_WI_3_evaluation']\n",
    "days_to_predict = [1920]\n",
    "wrmsse_score =  function2(input_data_points,days_to_predict)\n",
    "if wrmsse_score:\n",
    "    print(f\"WRMSSE score is {wrmsse_score}\")\n",
    "else:\n",
    "    print(\"We have data set with target values with in a range of 91 and 1941 so please give with in that range\")\n",
    "print(\"running time {:.3f} secs...\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thank you**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
